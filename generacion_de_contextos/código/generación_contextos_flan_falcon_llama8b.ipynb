{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1194530-e671-4df9-b12e-f529527c36fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.36.2)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2)\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.3.24-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.10.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: triton==2.1.0 in /opt/conda/lib/python3.10/site-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /opt/conda/lib/python3.10/site-packages (from torch) (2.18.1)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (2.8.6)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.3.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n",
      "Collecting langsmith<0.4,>=0.1.17\n",
      "  Downloading langsmith-0.3.38-py3-none-any.whl (359 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m359.3/359.3 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.2)\n",
      "Collecting langchain-core<1.0.0,>=0.3.55\n",
      "  Downloading langchain_core-0.3.56-py3-none-any.whl (437 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m437.2/437.2 kB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.4.41)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.8\n",
      "  Downloading langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
      "Collecting pydantic<3.0.0,>=2.7.4\n",
      "  Downloading pydantic-2.11.4-py3-none-any.whl (443 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.9/443.9 kB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2022.2.1)\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Collecting typing-extensions\n",
      "  Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m310.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting jsonpatch<2.0,>=1.33\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Collecting zstandard<0.24.0,>=0.23.0\n",
      "  Downloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting orjson<4.0.0,>=3.9.14\n",
      "  Downloading orjson-3.10.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.8/132.8 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting httpx<1,>=0.23.0\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m349.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests-toolbelt<2.0.0,>=1.0.0\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m343.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydantic-core==2.33.2\n",
      "  Downloading pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting annotated-types>=0.6.0\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting typing-inspection>=0.4.0\n",
      "  Downloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2022.6.15.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (1.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.2.1)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (3.6.1)\n",
      "Collecting httpcore==1.*\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m380.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting h11>=0.16\n",
      "  Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Collecting jsonpointer>=1.9\n",
      "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.0)\n",
      "Installing collected packages: zstandard, typing-extensions, tenacity, orjson, jsonpointer, h11, annotated-types, typing-inspection, requests-toolbelt, pydantic-core, jsonpatch, httpcore, pydantic, httpx, langsmith, langchain-core, langchain-text-splitters, langchain\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.3.0\n",
      "    Uninstalling typing_extensions-4.3.0:\n",
      "      Successfully uninstalled typing_extensions-4.3.0\n",
      "Successfully installed annotated-types-0.7.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.24 langchain-core-0.3.56 langchain-text-splitters-0.3.8 langsmith-0.3.38 orjson-3.10.18 pydantic-2.11.4 pydantic-core-2.33.2 requests-toolbelt-1.0.0 tenacity-9.1.2 typing-extensions-4.13.2 typing-inspection-0.4.0 zstandard-0.23.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch langchain pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "978c7274-0302-4453-8ae2-0e375775c40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.36.2)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.64.1)\n",
      "Collecting tokenizers<0.22,>=0.21\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0\n",
      "  Downloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.4/481.4 kB\u001b[0m \u001b[31m142.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Collecting safetensors>=0.4.3\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m217.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2023.10.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2022.6.15.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3)\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: safetensors\n",
      "    Found existing installation: safetensors 0.4.1\n",
      "    Uninstalling safetensors-0.4.1:\n",
      "      Successfully uninstalled safetensors-0.4.1\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.20.2\n",
      "    Uninstalling huggingface-hub-0.20.2:\n",
      "      Successfully uninstalled huggingface-hub-0.20.2\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.15.0\n",
      "    Uninstalling tokenizers-0.15.0:\n",
      "      Successfully uninstalled tokenizers-0.15.0\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.36.2\n",
      "    Uninstalling transformers-4.36.2:\n",
      "      Successfully uninstalled transformers-4.36.2\n",
      "Successfully installed huggingface-hub-0.30.2 safetensors-0.5.3 tokenizers-0.21.1 transformers-4.51.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89c816f7-4706-4bfd-aa02-94d2f8b4bf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b8ab2f-8c08-4eb6-80c7-41fcc3681745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cbd9e3e0c3340c898f007671fe0a491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "\n",
    "# Ruta a tus archivos\n",
    "input_csv = \"terminos_contextos.csv\"\n",
    "output_csv = \"terminos_contexto_generado.csv\"\n",
    "\n",
    "# Cargar CSV\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "# Inicializar pipeline directamente (asegúrate de tener acceso al modelo y usa el token si es necesario)\n",
    "modelo_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"nvidia/Llama-3.1-Nemotron-Nano-8B-v1\",\n",
    "    device=0  # Usa 0 para GPU o -1 para CPU\n",
    ")\n",
    "\n",
    "# Generar contexto\n",
    "def generar_ventana(termino, contextos):\n",
    "    prompt = (\n",
    "        f\"Eres un experto en lingüística jurídica.\\n\"\n",
    "        f\"Dado el conjunto de contextos de uso para el término \\\"{termino}\\\", sintetiza una ventana de contexto clara \"\n",
    "        f\"y precisa en español, de máximo 100 palabras.\\n\\n\"\n",
    "        f\"Contextos:\\n{contextos}\\n\\n\"\n",
    "        f\"Respuesta:\"\n",
    "    )\n",
    "    try:\n",
    "        # Generación de texto con el modelo\n",
    "        output = modelo_pipeline(prompt, max_new_tokens=200)[0][\"generated_text\"]\n",
    "        return output.split(\"Respuesta:\")[-1].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error con '{termino}': {e}\")\n",
    "        return \"Error en la generación\"\n",
    "\n",
    "# Aplicar la función a cada término y contexto\n",
    "resultados = []\n",
    "for _, row in df.iterrows():\n",
    "    termino = row['Termino']\n",
    "    contextos = \" \".join([str(row[col]) for col in df.columns if 'contexto' in col.lower() and pd.notna(row[col])])\n",
    "    resultados.append({\n",
    "        \"Termino\": termino,\n",
    "        \"Ventana_de_contexto\": generar_ventana(termino, contextos)\n",
    "    })\n",
    "\n",
    "# Guardar los resultados en un nuevo archivo CSV\n",
    "pd.DataFrame(resultados).to_csv(output_csv, index=False, encoding=\"utf-8\")\n",
    "print(\"✅ Contextos generados.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f1645d-5ce8-45f2-b5ce-23a00b92f4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f690a2-9ee9-48c2-a124-fdbfd5a4b106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "# Leer CSV\n",
    "input_csv = 'terminos_contextos.csv'\n",
    "output_csv = 'terminos_ventanas_contexto_flan.csv'\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "# Cargar modelo T5\n",
    "modelo_pipeline = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"google/flan-t5-base\",\n",
    "    device=0\n",
    ")\n",
    "\n",
    "# Construir prompt (igual que antes)\n",
    "def construir_prompt(termino, contextos):\n",
    "    return f\"\"\" \n",
    "Eres un experto en lingüística jurídica.\n",
    "Dado el conjunto de contextos de uso para el término \"{termino}\",\n",
    "sintetiza una ventana de contexto de máximo 100 palabras que explique claramente el significado y uso del término,\n",
    "con el objetivo de facilitar su enlazado con otros conceptos.\n",
    "\n",
    "Contextos:\n",
    "{contextos}\n",
    "\"\"\"\n",
    "\n",
    "# Función para generar ventana de contexto\n",
    "def generar_ventana_contexto(termino, contextos):\n",
    "    prompt = construir_prompt(termino, contextos)\n",
    "    try:\n",
    "        respuesta = modelo_pipeline(prompt, max_new_tokens=150)[0]['generated_text']\n",
    "        return respuesta.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error al generar para '{termino}': {e}\")\n",
    "        return \"Error en la generación\"\n",
    "\n",
    "# Procesar todos los términos\n",
    "resultados = []\n",
    "for _, row in df.iterrows():\n",
    "    termino = row['Termino']\n",
    "    contextos = \" \".join([str(row[col]) for col in df.columns if col.lower().startswith('contexto') and pd.notna(row[col])])\n",
    "    if not contextos.strip():\n",
    "        continue\n",
    "    ventana = generar_ventana_contexto(termino, contextos)\n",
    "    resultados.append({\n",
    "        \"Termino\": termino,\n",
    "        \"Ventana_de_contexto\": ventana\n",
    "    })\n",
    "\n",
    "# Guardar resultados\n",
    "df_resultado = pd.DataFrame(resultados)\n",
    "df_resultado.to_csv(output_csv, index=False, encoding='utf-8')\n",
    "print(f\"✅ Contextos guardados en {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e83e1b2-572f-4ad2-ba08-2436c51c1950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "# Leer CSV\n",
    "input_csv = 'terminos_contextos.csv'\n",
    "output_csv = 'terminos_ventanas_contexto_falcon.csv'\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "# Cargar modelo Falcon\n",
    "modelo_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"tiiuae/falcon-rw-1b\",\n",
    "    device=0  # Usa -1 si no tienes GPU\n",
    ")\n",
    "\n",
    "# Construir prompt adaptado a Falcon (causal language model)\n",
    "def construir_prompt(termino, contextos):\n",
    "    return f\"\"\"Eres un experto en lingüística jurídica.\n",
    "Dado el conjunto de contextos de uso para el término \"{termino}\", sintetiza una ventana de contexto clara y precisa (máximo 100 palabras)\n",
    "que explique su significado y uso, para facilitar su enlazado con otros conceptos.\n",
    "\n",
    "Contextos:\n",
    "{contextos}\n",
    "\n",
    "Respuesta:\"\"\"\n",
    "\n",
    "# Generar ventana de contexto\n",
    "def generar_ventana_contexto(termino, contextos):\n",
    "    prompt = construir_prompt(termino, contextos)\n",
    "    try:\n",
    "        respuesta = modelo_pipeline(prompt, max_new_tokens=150, do_sample=True, temperature=0.7)[0]['generated_text']\n",
    "        # Extraer solo la respuesta después de \"Respuesta:\"\n",
    "        return respuesta.split(\"Respuesta:\")[-1].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error al generar para '{termino}': {e}\")\n",
    "        return \"Error en la generación\"\n",
    "\n",
    "# Procesar términos\n",
    "resultados = []\n",
    "for _, row in df.iterrows():\n",
    "    termino = row['Termino']\n",
    "    contextos = \" \".join([str(row[col]) for col in df.columns if col.lower().startswith('contexto') and pd.notna(row[col])])\n",
    "    if not contextos.strip():\n",
    "        continue\n",
    "    ventana = generar_ventana_contexto(termino, contextos)\n",
    "    resultados.append({\n",
    "        \"Termino\": termino,\n",
    "        \"Ventana_de_contexto\": ventana\n",
    "    })\n",
    "\n",
    "# Guardar resultados\n",
    "df_resultado = pd.DataFrame(resultados)\n",
    "df_resultado.to_csv(output_csv, index=False, encoding='utf-8')\n",
    "print(f\"✅ Contextos guardados en {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e4bbbc1-c274-4302-810d-f03a0fb88ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 08:07:26.447240: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dc0d8d204294502896e33d9ccf18614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Contextos generados.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "\n",
    "# Ruta a tus archivos\n",
    "input_csv = \"terminos_contextos.csv\"\n",
    "output_csv = \"terminos_contexto_generado_1shot.csv\"\n",
    "\n",
    "# Cargar CSV\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "# Inicializar pipeline directamente\n",
    "modelo_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"nvidia/Llama-3.1-Nemotron-Nano-8B-v1\",\n",
    "    device=-1\n",
    ")\n",
    "\n",
    "# Generar contexto\n",
    "def generar_ventana(termino, contextos):\n",
    "    prompt = (\n",
    "        \n",
    "\n",
    "  f\"Eres un experto en lingüística jurídica.\\n\"\n",
    "        f\"Dado el conjunto de contextos de uso para el término \\\"{termino}\\\", sintetiza una ventana de contexto clara \"\n",
    "        f\"y precisa en español, de máximo 100 palabras, con el objetivo de facilitar su enlazado con otros conceptos.\\n\\n\"\n",
    "        f\"Ejemplo 1:\\n\"\n",
    "        f\"Término: trabajo nocturno\\n\"\n",
    "        f\"Contextos:\\n\"\n",
    "        f\"Artículo 36. Trabajo nocturno, trabajo a turnos y ritmo de trabajo.\\n\"\n",
    "        f\"nocturno, trabajo a turnos y ritmo de trabajo. 1. A los efectos de lo dispuesto en esta ley, se considera trabajo nocturno el realizado entre las diez de la noche y las seis de la mañana.\\n\"\n",
    "        f\"Respuesta:\\n\"\n",
    "        f\"El término 'trabajo nocturno' hace referencia al trabajo realizado durante el período comprendido entre las diez de la noche y las seis de la mañana, según lo estipulado por la legislación. Esta definición se incluye en el 'Artículo 36', donde también se menciona que el trabajo nocturno es un tipo de trabajo que se realiza en un horario que puede afectar el ritmo de vida del trabajador. Es relevante en el contexto laboral, especialmente en relación con las condiciones específicas y los derechos asociados a los trabajadores que realizan este tipo de jornada.\\n\\n\"\n",
    "        f\"Término: {termino}\\n\"\n",
    "        f\"Contextos:\\n{contextos}\\n\\n\"\n",
    "        f\"Respuesta:\"\n",
    "    )\n",
    "\n",
    "    \n",
    "    try:\n",
    "        output = modelo_pipeline(prompt, max_new_tokens=200)[0][\"generated_text\"]\n",
    "        return output.split(\"Respuesta:\")[-1].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error con '{termino}': {e}\")\n",
    "        return \"Error en la generación\"\n",
    "\n",
    "# Aplicar\n",
    "resultados = []\n",
    "for _, row in df.iterrows():\n",
    "    termino = row['Termino']\n",
    "    contextos = \" \".join([str(row[col]) for col in df.columns if 'contexto' in col.lower() and pd.notna(row[col])])\n",
    "    resultados.append({\n",
    "        \"Termino\": termino,\n",
    "        \"Ventana_de_contexto\": generar_ventana(termino, contextos)\n",
    "    })\n",
    "\n",
    "# Guardar resultados\n",
    "pd.DataFrame(resultados).to_csv(output_csv, index=False, encoding=\"utf-8\")\n",
    "print(\"✅ Contextos generados.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e372eed6-174a-4857-b395-5534c916c907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "485065ccf18c49eb980c575feb461565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Contextos generados.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "\n",
    "# Ruta a tus archivos\n",
    "input_csv = \"terminos_contextos.csv\"\n",
    "output_csv = \"terminos_contexto_generado_3shot.csv\"\n",
    "\n",
    "# Cargar CSV\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "# Inicializar pipeline directamente\n",
    "modelo_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"nvidia/Llama-3.1-Nemotron-Nano-8B-v1\",\n",
    "    device=-1\n",
    ")\n",
    "\n",
    "# Generar contexto\n",
    "def generar_ventana(termino, contextos):\n",
    "    prompt = (\n",
    "        \n",
    "\n",
    "  f\"Eres un experto en lingüística jurídica.\\n\"\n",
    "        f\"Dado el conjunto de contextos de uso para el término \\\"{termino}\\\", sintetiza una ventana de contexto clara \"\n",
    "        f\"y precisa en español, de máximo 100 palabras, con el objetivo de facilitar su enlazado con otros conceptos.\\n\\n\"\n",
    "        f\"Ejemplo 1:\\n\"\n",
    "        f\"Término: trabajo nocturno\\n\"\n",
    "        f\"Contextos:\\n\"\n",
    "        f\"Artículo 36. Trabajo nocturno, trabajo a turnos y ritmo de trabajo.\\n\"\n",
    "        f\"nocturno, trabajo a turnos y ritmo de trabajo. 1. A los efectos de lo dispuesto en esta ley, se considera trabajo nocturno el realizado entre las diez de la noche y las seis de la mañana.\\n\"\n",
    "        f\"Respuesta:\\n\"\n",
    "        f\"El término 'trabajo nocturno' hace referencia al trabajo realizado durante el período comprendido entre las diez de la noche y las seis de la mañana, según lo estipulado por la legislación. Esta definición se incluye en el 'Artículo 36', donde también se menciona que el trabajo nocturno es un tipo de trabajo que se realiza en un horario que puede afectar el ritmo de vida del trabajador. Es relevante en el contexto laboral, especialmente en relación con las condiciones específicas y los derechos asociados a los trabajadores que realizan este tipo de jornada.\\n\\n\"\n",
    "        f\"Ejemplo 2:\\n\"\n",
    "    f\"Término: maternidad\\n\"\n",
    "    f\"Contextos:\\n\"\n",
    "    f\"fecha de inicio del embarazo hasta el comienzo del período de suspensión a que se refiere el artículo 48.4, o maternidad , salvo que concurran motivos no relacionados con el embarazo o maternidad.\\n\"\n",
    "    f\"Respuesta:\\n\"\n",
    "    f\"El término 'maternidad' se refiere a la situación legal y laboral derivada del embarazo y parto, durante la cual la trabajadora tiene derecho a una suspensión del contrato para garantizar su salud y la del recién nacido. Esta suspensión se regula en el artículo 48.4 del Estatuto de los Trabajadores y puede verse acompañada de protecciones adicionales frente a despidos y modificaciones de condiciones laborales motivadas por esta situación.\\n\\n\"\n",
    "\n",
    "    f\"Ejemplo 3:\\n\"\n",
    "    f\"Término: despido colectivo\\n\"\n",
    "    f\"Contextos:\\n\"\n",
    "    f\"de trabajo, siempre que su existencia haya sido debidamente constatada conforme a lo dispuesto en el artículo 51.7. i) Por despido colectivo fundado en causas económicas, técnicas, organizativas o de producción.\\n\"\n",
    "    f\"Artículo 51. Despido colectivo.\\n\"\n",
    "    f\"Artículo 51. Despido colectivo. 1. A efectos de lo dispuesto en esta ley se entenderá por despido colectivo la extinción de contratos de trabajo fundada en causas económicas, técnicas, organizativas o de producción cuando, en un periodo de noventa días, la extinción afecte al menos a: a) Diez trabajadores, en las empresas que ocupen menos de cien trabajadores.\\n\"\n",
    "    f\"en la demanda de los productos o servicios que la empresa pretende colocar en el mercado.\\n\"\n",
    "    f\"Respuesta:\\n\"\n",
    "    f\"El término 'despido colectivo' se refiere a la terminación simultánea de múltiples contratos de trabajo por causas económicas, técnicas, organizativas o de producción. Según el artículo 51 del Estatuto de los Trabajadores, se considera colectivo cuando afecta a un número determinado de empleados dentro de un período de noventa días. Este tipo de despido requiere procedimientos específicos, incluida la negociación con los representantes de los trabajadores y la autorización administrativa en algunos casos.\\n\\n\"\n",
    "\n",
    "    f\"Término: {{termino}}\\n\"\n",
    "        f\"Término: {termino}\\n\"\n",
    "        f\"Contextos:\\n{contextos}\\n\\n\"\n",
    "        f\"Respuesta:\"\n",
    "    )\n",
    "\n",
    "    \n",
    "    try:\n",
    "        output = modelo_pipeline(prompt, max_new_tokens=200)[0][\"generated_text\"]\n",
    "        return output.split(\"Respuesta:\")[-1].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error con '{termino}': {e}\")\n",
    "        return \"Error en la generación\"\n",
    "\n",
    "# Aplicar\n",
    "resultados = []\n",
    "for _, row in df.iterrows():\n",
    "    termino = row['Termino']\n",
    "    contextos = \" \".join([str(row[col]) for col in df.columns if 'contexto' in col.lower() and pd.notna(row[col])])\n",
    "    resultados.append({\n",
    "        \"Termino\": termino,\n",
    "        \"Ventana_de_contexto\": generar_ventana(termino, contextos)\n",
    "    })\n",
    "\n",
    "# Guardar resultados\n",
    "pd.DataFrame(resultados).to_csv(output_csv, index=False, encoding=\"utf-8\")\n",
    "print(\"✅ Contextos generados.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed3ce58-a5b2-4819-8d9b-0f267c6128b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
